{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023/05 mjke\n",
    "\n",
    "Motivated by: https://www.haihai.ai/gpt-gdrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = '../data/'\n",
    "\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html\n",
    "DIR_CHROMA_DB = f'{DIR_DATA}/chroma'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain contains lots of loaders at: https://python.langchain.com/en/latest/modules/indexes/document_loaders.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gdrive version\n",
    "\n",
    "#from langchain.document_loaders import GoogleDriveLoader\n",
    "\n",
    "#folder_id = \"YOUR_FOLDER_ID\"\n",
    "#loader = GoogleDriveLoader(\n",
    "#    folder_id=folder_id,\n",
    "#    recursive=False\n",
    "#)\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, UnstructuredWordDocumentLoader\n",
    "\n",
    "loader = DirectoryLoader(DIR_DATA, glob=\"*.docx\", use_multithreading=True, show_progress=True,\n",
    "                         loader_cls=UnstructuredWordDocumentLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding choice (chromadb defaults to `text-embedding-ada-002`)\n",
    "- https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "- https://docs.trychroma.com/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=DIR_CHROMA_DB) # most expensive step\n",
    "retriever = db.as_retriever()\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study used several deep learning models, including a fully connected \"multi-layer perceptron\" (Twin-NN), a DenseNet 32 architecture (Twin-DN), and recurrent architectures such as LSTM and GRU. They also used deep metric learning models for identifying drug replicates inducing noticeable but subtle changes in zebrafish behavior.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(\"what kind of deep learning did they use?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, it seems that the paper is related to drug discovery and deep learning models for behavioral screening data. Some potential scientific journals that may be a good fit for this paper could include Journal of Medicinal Chemistry, Nature Methods, or Drug Discovery Today. However, the final decision on which journal to submit to should be based on the specific focus and scope of the paper, as well as the target audience.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(\"what scientific journal would be good for this paper?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 zebrafish per well in the screening platform.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(\"how many zebrafish are there per well in the screen?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, the first screen was not randomized. The unintended outcome of the first metric learning models exploiting \"shortcut learning\" on the original dataset, which had not used randomized plate layouts, led to the need for a second high-replicate screen of NT-650, but this time with the treatments fully robotically randomized across plates and wells.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(\"was the first screen randomized?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, deep metric learning had limitations. One of the limitations was that the models could fall prey to \"shortcut learning,\" where they exploited experimental artifacts in the screening dataset to achieve misleading performance that did not generalize to similar but independent zebrafish screens. This deep mis-learning was nuanced and eluded conventional cross-validation, sanity checks, and exploratory data analysis tests. \n",
      "\n",
      "Regarding the simple or dense model, the Twin-DN model was more computationally expressive than the Twin-NN model, but it was found to readily memorize the high-frequency components in the time series, which may come from artifacts such as plate vibrations or high-frequency noise in the imaging sensor. This resulted in the model's exceptional performance relying on shortcut learning or the exploitation of hidden artifactual cues encoded within the data that were invisible to human researchers but perceivable by the deep learning model.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(\"did deep metric learning have any limitations? did it matter whether they used the simple or dense model?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
